# base config: dinov2/configs/train/vitl14.yaml
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32

dino:
  # head_n_prototypes: 131072
  head_bottleneck_dim: 256  # changed for vision mamba (384)
ibot:
  separate_head: true
  # head_n_prototypes: 131072
train:
  batch_size_per_gpu: 192
  num_workers: 26
  OFFICIAL_EPOCH_LENGTH: 500  # 1250
  dataset_path: PatchDataset:root=/lustre/groups/shared/histology_data/patch_lists/train_new.txt  # original: ImageNet22k
  centering: sinkhorn_knopp
student:
  arch: vit_small
  patch_size: 16  # changed for vision mamba (14)
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0  # for distributed training
  num_register_tokens: 0 # 0 for no register tokens

teacher:
  momentum_teacher: 0.994
optim:
  epochs: 200  # 500
  weight_decay_end: 0.2
  base_lr: 2.0e-04  # learning rate for a batch size of 1024
  warmup_epochs: 10  # 80
  layerwise_decay: 1.0
crops:
  local_crops_size: 96  # changed for vision mamba (98)
