# base config: dinov2/configs/train/vitl14.yaml
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32


ibot:
  separate_head: true
  # head_n_prototypes: 131072
train:
  batch_size_per_gpu: 256  # 40 for vitl on 40GB A100, 256 for vits on 80GB A100, 144 for vits on 40GB A100
  num_workers: 20
  OFFICIAL_EPOCH_LENGTH: 3125  # 1250 # should be actual dataset size / batch_size_per_gpu / num_gpus
  # dataset_path: BalancedPatchDataset:root=/lustre/groups/shared/histology_data/patch_lists/train_lists:shuffle=1  # length 10M, OFFICIAL_EPOCH_LENGTH: 8740, epochs: 10
  # dataset_path: PatchDataset
  # finetune on NCT-CRC-100K dataset
  # dataset_path: PatchDataset:root=/lustre/groups/shared/histology_data/patch_lists/NCT-CRC-100K.txt:shuffle=1  # length 100K, OFFICIAL_EPOCH_LENGTH: 200, epochs: 25
  # finetune on TCGA-CRC dataset 
  # dataset_path: PatchDataset:root=/lustre/groups/shared/histology_data/TCGA/CRC/patches/512px_crc_wonorm_complete_diag.txt:shuffle=1  # length: 1.6M, OFFICIAL_EPOCH_LENGTH: 3125, epochs: 10
  # finetune on TCGA-CRC 1024 patches (diag and frozen)
  dataset_path: PatchDataset:root=/lustre/groups/shared/histology_data/patch_lists/tcga_patches_coad_read.txt:shuffle=1 # length: 400K (*4), OFFICIAL_EPOCH_LENGTH: 3125, epochs: 10

  centering: sinkhorn_knopp

  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 0  # for distributed training
  num_register_tokens: 0  # 0 for no register tokens

teacher:
  momentum_teacher: 0.994
optim:
  epochs: 10  # 500
  weight_decay_end: 0.2
  base_lr: 2.0e-04  # learning rate for a batch size of 1024
  warmup_epochs: 10  # 80
  layerwise_decay: 1.0


evaluation:
  eval_period_iterations: 5000


# adapt to model architecture
# ---------------------------
# config for vit
# "dinov2_vits14","dinov2_vitb14","dinov2_vitl14","dinov2_vitg14"
# 384, 768, 1024, 1536
student:
  arch: dinov2_vits14
  patch_size: 14
crops:
  local_crops_size: 98
dino:
  head_bottleneck_dim: 384 
  smooth_rank_loss_weight: 0.0

# ---------------------------
# config for vim_tiny
#student:
#  arch: vim_tiny
#  patch_size: 16
#crops:
#  local_crops_size: 96
#dino:
#  head_bottleneck_dim: 256
